2025-04-17 09:42:18,535 - mmseg - INFO - Multi-processing start method is `None`
2025-04-17 09:42:18,606 - mmseg - INFO - OpenCV num_threads is `1
2025-04-17 09:42:18,607 - mmseg - INFO - OMP num threads is 1
2025-04-17 09:42:18,718 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
CUDA available: True
GPU 0: NVIDIA A100 80GB PCIe
CUDA_HOME: /appl/cuda/11.3.0
NVCC: Cuda compilation tools, release 11.3, V11.3.58
GCC: gcc (GCC) 12.3.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.6.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.29.0+45d03f6
------------------------------------------------------------

2025-04-17 09:42:18,719 - mmseg - INFO - Distributed training: False
2025-04-17 09:42:19,062 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='VPDSeg',
    pretrained='open-mmlab://resnet50_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 1, 1),
        strides=(1, 2, 2, 2),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        style='pytorch',
        contract_dilation=True),
    neck=dict(
        type='FPN',
        in_channels=[320, 790, 1430, 1280],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=256,
        dropout_ratio=0.1,
        num_classes=150,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='slide', crop_size=(512, 512), stride=(341, 341)),
    sd_path='/work3/s203557/checkpoints/v1-5-pruned-emaonly.ckpt',
    sd_config=
    '/zhome/b6/d/154958/ADLCV_Project/VPD/segmentation/v1-inference.yaml')
dataset_type = 'ADE20KDataset'
data_root = '/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016'
IMG_MEAN = [127.5, 127.5, 127.5]
IMG_VAR = [127.5, 127.5, 127.5]
img_norm_cfg = dict(
    mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[127.5, 127.5, 127.5],
        std=[127.5, 127.5, 127.5],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='GenerateBoundingBoxMasksFromSeg'),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg', 'gt_bbox_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=8,
    train=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='GenerateBoundingBoxMasksFromSeg'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_bbox_masks'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/work3/s203557/checkpoints/vpd.chkpt'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
find_unused_parameters = True
optimizer = dict(
    type='AdamW',
    lr=8e-05,
    weight_decay=0.001,
    paramwise_cfg=dict(
        custom_keys=dict(
            unet=dict(lr_mult=0.1),
            encoder_vq=dict(lr_mult=0.0),
            text_encoder=dict(lr_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    power=1,
    min_lr=0.0,
    by_epoch=False,
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=8000)
evaluation = dict(interval=8000, metric='mIoU')
work_dir = './work_dirs/vpd_config'
gpu_ids = [0]
auto_resume = False

2025-04-17 09:42:19,062 - mmseg - INFO - Set random seed to 1909104303, deterministic: False
2025-04-17 09:42:29,339 - mmseg - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-17 09:42:29,357 - mmseg - INFO - initialize FPNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

gamma - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_in.weight - torch.Size([128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_in.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.downsample.conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.downsample.conv.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv1.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.nin_shortcut.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.nin_shortcut.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.downsample.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.downsample.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv1.weight - torch.Size([512, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.nin_shortcut.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.nin_shortcut.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.downsample.conv.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.downsample.conv.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.q.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.q.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.k.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.k.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.v.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.v.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.proj_out.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.proj_out.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.norm_out.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.norm_out.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_out.weight - torch.Size([8, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_out.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.quant_conv.weight - torch.Size([8, 8, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.quant_conv.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.post_quant_conv.weight - torch.Size([4, 4, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.post_quant_conv.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.0.weight - torch.Size([1280, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.2.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.0.0.weight - torch.Size([320, 4, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.3.0.op.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.3.0.op.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.weight - torch.Size([640, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.6.0.op.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.6.0.op.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.weight - torch.Size([1280, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.9.0.op.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.9.0.op.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight - torch.Size([1280, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.weight - torch.Size([1280, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight - torch.Size([640, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.weight - torch.Size([640, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight - torch.Size([640, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.weight - torch.Size([640, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight - torch.Size([640, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.weight - torch.Size([640, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight - torch.Size([320, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.weight - torch.Size([320, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.2.weight - torch.Size([4, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.2.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.0.weight - torch.Size([1280, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.2.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.0.0.weight - torch.Size([320, 4, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.3.0.op.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.3.0.op.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.weight - torch.Size([640, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.6.0.op.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.6.0.op.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.weight - torch.Size([1280, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.9.0.op.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.9.0.op.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight - torch.Size([1280, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.weight - torch.Size([1280, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight - torch.Size([640, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.weight - torch.Size([640, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight - torch.Size([640, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.weight - torch.Size([640, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight - torch.Size([640, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.weight - torch.Size([640, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight - torch.Size([320, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.weight - torch.Size([320, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.2.weight - torch.Size([4, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.2.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.0.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.2.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.3.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.4.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.4.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.5.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.5.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out64.0.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out64.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out32.0.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out32.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out16.0.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out16.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.2.weight - torch.Size([320, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.4.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv32.0.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv32.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv16.0.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv16.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.2.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.0.conv.weight - torch.Size([256, 320, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.1.conv.weight - torch.Size([256, 790, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.2.conv.weight - torch.Size([256, 1430, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.3.conv.weight - torch.Size([256, 1280, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.conv_seg.weight - torch.Size([150, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([150]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.scale_heads.0.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.0.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.0.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  
2025-04-17 09:42:29,379 - mmseg - INFO - VPDSeg(
  (encoder_vq): AutoencoderKL(
    (encoder): Encoder(
      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (3): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): Identity()
    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
  )
  (unet): UNetWrapper(
    (frozen_unet): DiffusionWrapper(
      (diffusion_model): UNetModel(
        (time_embed): Sequential(
          (0): Linear(in_features=320, out_features=1280, bias=True)
          (1): SiLU()
          (2): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (input_blocks): ModuleList(
          (0): TimestepEmbedSequential(
            (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (1): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (2): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (3): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (4): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (5): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (6): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (7): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (8): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (9): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (10): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
          (11): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
        )
        (middle_block): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=768, out_features=1280, bias=False)
                  (to_v): Linear(in_features=768, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
        )
        (output_blocks): ModuleList(
          (0): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (1): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (2): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): Upsample(
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (3): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (4): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (5): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (2): Upsample(
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (6): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (7): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (8): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (2): Upsample(
              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (9): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (10): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (11): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
        )
        (out): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (trainable_unet): DiffusionWrapper(
      (diffusion_model): UNetModel(
        (time_embed): Sequential(
          (0): Linear(in_features=320, out_features=1280, bias=True)
          (1): SiLU()
          (2): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (input_blocks): ModuleList(
          (0): TimestepEmbedSequential(
            (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (1): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (2): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (3): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (4): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (5): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (6): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (7): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (8): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (9): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (10): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
          (11): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
        )
        (middle_block): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): CrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=768, out_features=1280, bias=False)
                  (to_v): Linear(in_features=768, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
        )
        (output_blocks): ModuleList(
          (0): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (1): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (2): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): Upsample(
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (3): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (4): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (5): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=768, out_features=1280, bias=False)
                    (to_v): Linear(in_features=768, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (2): Upsample(
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (6): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (7): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (8): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=768, out_features=640, bias=False)
                    (to_v): Linear(in_features=768, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (2): Upsample(
              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (9): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (10): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (11): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): CrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=768, out_features=320, bias=False)
                    (to_v): Linear(in_features=768, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
          )
        )
        (out): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (zero_convs): ModuleList(
      (0): ZeroConv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
      (1): ZeroConv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
      (2): ZeroConv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
      (3): ZeroConv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
      (4): ZeroConv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
      (5): ZeroConv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (box_encoder): EncoderControlNet(
    (out64): Sequential(
      (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (out32): Sequential(
      (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (out16): Sequential(
      (0): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv64): Sequential(
      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (5): ReLU(inplace=True)
    )
    (conv32): Sequential(
      (0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv16): Sequential(
      (0): Conv2d(640, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
  )
  (sd_model): LatentDiffusion(
    (model): None
    (first_stage_model): None
  )
  (text_adapter): TextAdapter(
    (fc): Sequential(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): GELU()
      (2): Linear(in_features=768, out_features=768, bias=True)
    )
  )
  (neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(790, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(1430, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (decode_head): FPNHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (scale_heads): ModuleList(
      (0): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (1): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
      )
      (2): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
        (2): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): Upsample()
      )
      (3): Sequential(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Upsample()
        (2): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (3): Upsample()
        (4): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (5): Upsample()
      )
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2025-04-17 09:42:29,809 - mmseg - INFO - Loaded 20210 images
2025-04-17 09:42:36,190 - mmseg - INFO - Loaded 2000 images
2025-04-17 09:42:36,191 - mmseg - INFO - load checkpoint from local path: /work3/s203557/checkpoints/vpd.chkpt
2025-04-17 09:42:37,947 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: unet.unet.diffusion_model.time_embed.0.weight, unet.unet.diffusion_model.time_embed.0.bias, unet.unet.diffusion_model.time_embed.2.weight, unet.unet.diffusion_model.time_embed.2.bias, unet.unet.diffusion_model.input_blocks.0.0.weight, unet.unet.diffusion_model.input_blocks.0.0.bias, unet.unet.diffusion_model.input_blocks.1.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.1.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.1.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.1.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.1.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.1.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.1.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.1.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.1.1.norm.weight, unet.unet.diffusion_model.input_blocks.1.1.norm.bias, unet.unet.diffusion_model.input_blocks.1.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.1.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.1.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.1.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.2.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.2.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.2.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.2.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.2.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.2.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.2.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.2.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.2.1.norm.weight, unet.unet.diffusion_model.input_blocks.2.1.norm.bias, unet.unet.diffusion_model.input_blocks.2.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.2.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.2.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.2.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.3.0.op.weight, unet.unet.diffusion_model.input_blocks.3.0.op.bias, unet.unet.diffusion_model.input_blocks.4.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.4.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.4.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.4.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.4.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.4.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.4.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.4.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.4.0.skip_connection.weight, unet.unet.diffusion_model.input_blocks.4.0.skip_connection.bias, unet.unet.diffusion_model.input_blocks.4.1.norm.weight, unet.unet.diffusion_model.input_blocks.4.1.norm.bias, unet.unet.diffusion_model.input_blocks.4.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.4.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.4.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.4.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.5.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.5.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.5.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.5.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.5.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.5.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.5.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.5.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.5.1.norm.weight, unet.unet.diffusion_model.input_blocks.5.1.norm.bias, unet.unet.diffusion_model.input_blocks.5.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.5.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.5.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.5.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.6.0.op.weight, unet.unet.diffusion_model.input_blocks.6.0.op.bias, unet.unet.diffusion_model.input_blocks.7.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.7.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.7.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.7.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.7.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.7.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.7.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.7.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.7.0.skip_connection.weight, unet.unet.diffusion_model.input_blocks.7.0.skip_connection.bias, unet.unet.diffusion_model.input_blocks.7.1.norm.weight, unet.unet.diffusion_model.input_blocks.7.1.norm.bias, unet.unet.diffusion_model.input_blocks.7.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.7.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.7.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.7.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.8.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.8.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.8.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.8.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.8.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.8.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.8.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.8.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.8.1.norm.weight, unet.unet.diffusion_model.input_blocks.8.1.norm.bias, unet.unet.diffusion_model.input_blocks.8.1.proj_in.weight, unet.unet.diffusion_model.input_blocks.8.1.proj_in.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.input_blocks.8.1.proj_out.weight, unet.unet.diffusion_model.input_blocks.8.1.proj_out.bias, unet.unet.diffusion_model.input_blocks.9.0.op.weight, unet.unet.diffusion_model.input_blocks.9.0.op.bias, unet.unet.diffusion_model.input_blocks.10.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.10.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.10.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.10.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.10.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.10.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.10.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.10.0.out_layers.3.bias, unet.unet.diffusion_model.input_blocks.11.0.in_layers.0.weight, unet.unet.diffusion_model.input_blocks.11.0.in_layers.0.bias, unet.unet.diffusion_model.input_blocks.11.0.in_layers.2.weight, unet.unet.diffusion_model.input_blocks.11.0.in_layers.2.bias, unet.unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight, unet.unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias, unet.unet.diffusion_model.input_blocks.11.0.out_layers.0.weight, unet.unet.diffusion_model.input_blocks.11.0.out_layers.0.bias, unet.unet.diffusion_model.input_blocks.11.0.out_layers.3.weight, unet.unet.diffusion_model.input_blocks.11.0.out_layers.3.bias, unet.unet.diffusion_model.middle_block.0.in_layers.0.weight, unet.unet.diffusion_model.middle_block.0.in_layers.0.bias, unet.unet.diffusion_model.middle_block.0.in_layers.2.weight, unet.unet.diffusion_model.middle_block.0.in_layers.2.bias, unet.unet.diffusion_model.middle_block.0.emb_layers.1.weight, unet.unet.diffusion_model.middle_block.0.emb_layers.1.bias, unet.unet.diffusion_model.middle_block.0.out_layers.0.weight, unet.unet.diffusion_model.middle_block.0.out_layers.0.bias, unet.unet.diffusion_model.middle_block.0.out_layers.3.weight, unet.unet.diffusion_model.middle_block.0.out_layers.3.bias, unet.unet.diffusion_model.middle_block.1.norm.weight, unet.unet.diffusion_model.middle_block.1.norm.bias, unet.unet.diffusion_model.middle_block.1.proj_in.weight, unet.unet.diffusion_model.middle_block.1.proj_in.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.middle_block.1.proj_out.weight, unet.unet.diffusion_model.middle_block.1.proj_out.bias, unet.unet.diffusion_model.middle_block.2.in_layers.0.weight, unet.unet.diffusion_model.middle_block.2.in_layers.0.bias, unet.unet.diffusion_model.middle_block.2.in_layers.2.weight, unet.unet.diffusion_model.middle_block.2.in_layers.2.bias, unet.unet.diffusion_model.middle_block.2.emb_layers.1.weight, unet.unet.diffusion_model.middle_block.2.emb_layers.1.bias, unet.unet.diffusion_model.middle_block.2.out_layers.0.weight, unet.unet.diffusion_model.middle_block.2.out_layers.0.bias, unet.unet.diffusion_model.middle_block.2.out_layers.3.weight, unet.unet.diffusion_model.middle_block.2.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.0.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.0.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.0.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.0.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.0.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.0.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.0.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.0.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.0.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.0.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.1.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.1.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.1.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.1.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.1.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.1.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.1.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.1.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.1.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.1.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.2.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.2.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.2.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.2.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.2.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.2.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.2.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.2.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.2.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.2.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.2.1.conv.weight, unet.unet.diffusion_model.output_blocks.2.1.conv.bias, unet.unet.diffusion_model.output_blocks.3.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.3.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.3.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.3.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.3.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.3.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.3.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.3.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.3.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.3.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.3.1.norm.weight, unet.unet.diffusion_model.output_blocks.3.1.norm.bias, unet.unet.diffusion_model.output_blocks.3.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.3.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.3.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.3.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.4.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.4.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.4.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.4.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.4.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.4.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.4.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.4.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.4.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.4.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.4.1.norm.weight, unet.unet.diffusion_model.output_blocks.4.1.norm.bias, unet.unet.diffusion_model.output_blocks.4.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.4.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.4.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.4.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.5.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.5.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.5.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.5.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.5.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.5.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.5.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.5.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.5.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.5.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.5.1.norm.weight, unet.unet.diffusion_model.output_blocks.5.1.norm.bias, unet.unet.diffusion_model.output_blocks.5.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.5.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.5.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.5.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.5.2.conv.weight, unet.unet.diffusion_model.output_blocks.5.2.conv.bias, unet.unet.diffusion_model.output_blocks.6.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.6.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.6.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.6.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.6.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.6.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.6.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.6.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.6.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.6.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.6.1.norm.weight, unet.unet.diffusion_model.output_blocks.6.1.norm.bias, unet.unet.diffusion_model.output_blocks.6.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.6.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.6.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.6.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.7.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.7.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.7.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.7.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.7.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.7.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.7.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.7.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.7.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.7.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.7.1.norm.weight, unet.unet.diffusion_model.output_blocks.7.1.norm.bias, unet.unet.diffusion_model.output_blocks.7.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.7.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.7.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.7.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.8.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.8.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.8.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.8.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.8.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.8.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.8.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.8.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.8.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.8.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.8.1.norm.weight, unet.unet.diffusion_model.output_blocks.8.1.norm.bias, unet.unet.diffusion_model.output_blocks.8.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.8.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.8.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.8.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.8.2.conv.weight, unet.unet.diffusion_model.output_blocks.8.2.conv.bias, unet.unet.diffusion_model.output_blocks.9.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.9.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.9.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.9.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.9.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.9.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.9.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.9.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.9.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.9.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.9.1.norm.weight, unet.unet.diffusion_model.output_blocks.9.1.norm.bias, unet.unet.diffusion_model.output_blocks.9.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.9.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.9.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.9.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.10.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.10.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.10.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.10.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.10.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.10.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.10.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.10.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.10.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.10.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.10.1.norm.weight, unet.unet.diffusion_model.output_blocks.10.1.norm.bias, unet.unet.diffusion_model.output_blocks.10.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.10.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.10.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.10.1.proj_out.bias, unet.unet.diffusion_model.output_blocks.11.0.in_layers.0.weight, unet.unet.diffusion_model.output_blocks.11.0.in_layers.0.bias, unet.unet.diffusion_model.output_blocks.11.0.in_layers.2.weight, unet.unet.diffusion_model.output_blocks.11.0.in_layers.2.bias, unet.unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight, unet.unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias, unet.unet.diffusion_model.output_blocks.11.0.out_layers.0.weight, unet.unet.diffusion_model.output_blocks.11.0.out_layers.0.bias, unet.unet.diffusion_model.output_blocks.11.0.out_layers.3.weight, unet.unet.diffusion_model.output_blocks.11.0.out_layers.3.bias, unet.unet.diffusion_model.output_blocks.11.0.skip_connection.weight, unet.unet.diffusion_model.output_blocks.11.0.skip_connection.bias, unet.unet.diffusion_model.output_blocks.11.1.norm.weight, unet.unet.diffusion_model.output_blocks.11.1.norm.bias, unet.unet.diffusion_model.output_blocks.11.1.proj_in.weight, unet.unet.diffusion_model.output_blocks.11.1.proj_in.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight, unet.unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias, unet.unet.diffusion_model.output_blocks.11.1.proj_out.weight, unet.unet.diffusion_model.output_blocks.11.1.proj_out.bias, unet.unet.diffusion_model.out.0.weight, unet.unet.diffusion_model.out.0.bias, unet.unet.diffusion_model.out.2.weight, unet.unet.diffusion_model.out.2.bias, sd_model.lvlb_weights

missing keys in source state_dict: encoder_vq.encoder.conv_in.weight, encoder_vq.encoder.conv_in.bias, encoder_vq.encoder.down.0.block.0.norm1.weight, encoder_vq.encoder.down.0.block.0.norm1.bias, encoder_vq.encoder.down.0.block.0.conv1.weight, encoder_vq.encoder.down.0.block.0.conv1.bias, encoder_vq.encoder.down.0.block.0.norm2.weight, encoder_vq.encoder.down.0.block.0.norm2.bias, encoder_vq.encoder.down.0.block.0.conv2.weight, encoder_vq.encoder.down.0.block.0.conv2.bias, encoder_vq.encoder.down.0.block.1.norm1.weight, encoder_vq.encoder.down.0.block.1.norm1.bias, encoder_vq.encoder.down.0.block.1.conv1.weight, encoder_vq.encoder.down.0.block.1.conv1.bias, encoder_vq.encoder.down.0.block.1.norm2.weight, encoder_vq.encoder.down.0.block.1.norm2.bias, encoder_vq.encoder.down.0.block.1.conv2.weight, encoder_vq.encoder.down.0.block.1.conv2.bias, encoder_vq.encoder.down.0.downsample.conv.weight, encoder_vq.encoder.down.0.downsample.conv.bias, encoder_vq.encoder.down.1.block.0.norm1.weight, encoder_vq.encoder.down.1.block.0.norm1.bias, encoder_vq.encoder.down.1.block.0.conv1.weight, encoder_vq.encoder.down.1.block.0.conv1.bias, encoder_vq.encoder.down.1.block.0.norm2.weight, encoder_vq.encoder.down.1.block.0.norm2.bias, encoder_vq.encoder.down.1.block.0.conv2.weight, encoder_vq.encoder.down.1.block.0.conv2.bias, encoder_vq.encoder.down.1.block.0.nin_shortcut.weight, encoder_vq.encoder.down.1.block.0.nin_shortcut.bias, encoder_vq.encoder.down.1.block.1.norm1.weight, encoder_vq.encoder.down.1.block.1.norm1.bias, encoder_vq.encoder.down.1.block.1.conv1.weight, encoder_vq.encoder.down.1.block.1.conv1.bias, encoder_vq.encoder.down.1.block.1.norm2.weight, encoder_vq.encoder.down.1.block.1.norm2.bias, encoder_vq.encoder.down.1.block.1.conv2.weight, encoder_vq.encoder.down.1.block.1.conv2.bias, encoder_vq.encoder.down.1.downsample.conv.weight, encoder_vq.encoder.down.1.downsample.conv.bias, encoder_vq.encoder.down.2.block.0.norm1.weight, encoder_vq.encoder.down.2.block.0.norm1.bias, encoder_vq.encoder.down.2.block.0.conv1.weight, encoder_vq.encoder.down.2.block.0.conv1.bias, encoder_vq.encoder.down.2.block.0.norm2.weight, encoder_vq.encoder.down.2.block.0.norm2.bias, encoder_vq.encoder.down.2.block.0.conv2.weight, encoder_vq.encoder.down.2.block.0.conv2.bias, encoder_vq.encoder.down.2.block.0.nin_shortcut.weight, encoder_vq.encoder.down.2.block.0.nin_shortcut.bias, encoder_vq.encoder.down.2.block.1.norm1.weight, encoder_vq.encoder.down.2.block.1.norm1.bias, encoder_vq.encoder.down.2.block.1.conv1.weight, encoder_vq.encoder.down.2.block.1.conv1.bias, encoder_vq.encoder.down.2.block.1.norm2.weight, encoder_vq.encoder.down.2.block.1.norm2.bias, encoder_vq.encoder.down.2.block.1.conv2.weight, encoder_vq.encoder.down.2.block.1.conv2.bias, encoder_vq.encoder.down.2.downsample.conv.weight, encoder_vq.encoder.down.2.downsample.conv.bias, encoder_vq.encoder.down.3.block.0.norm1.weight, encoder_vq.encoder.down.3.block.0.norm1.bias, encoder_vq.encoder.down.3.block.0.conv1.weight, encoder_vq.encoder.down.3.block.0.conv1.bias, encoder_vq.encoder.down.3.block.0.norm2.weight, encoder_vq.encoder.down.3.block.0.norm2.bias, encoder_vq.encoder.down.3.block.0.conv2.weight, encoder_vq.encoder.down.3.block.0.conv2.bias, encoder_vq.encoder.down.3.block.1.norm1.weight, encoder_vq.encoder.down.3.block.1.norm1.bias, encoder_vq.encoder.down.3.block.1.conv1.weight, encoder_vq.encoder.down.3.block.1.conv1.bias, encoder_vq.encoder.down.3.block.1.norm2.weight, encoder_vq.encoder.down.3.block.1.norm2.bias, encoder_vq.encoder.down.3.block.1.conv2.weight, encoder_vq.encoder.down.3.block.1.conv2.bias, encoder_vq.encoder.mid.block_1.norm1.weight, encoder_vq.encoder.mid.block_1.norm1.bias, encoder_vq.encoder.mid.block_1.conv1.weight, encoder_vq.encoder.mid.block_1.conv1.bias, encoder_vq.encoder.mid.block_1.norm2.weight, encoder_vq.encoder.mid.block_1.norm2.bias, encoder_vq.encoder.mid.block_1.conv2.weight, encoder_vq.encoder.mid.block_1.conv2.bias, encoder_vq.encoder.mid.attn_1.norm.weight, encoder_vq.encoder.mid.attn_1.norm.bias, encoder_vq.encoder.mid.attn_1.q.weight, encoder_vq.encoder.mid.attn_1.q.bias, encoder_vq.encoder.mid.attn_1.k.weight, encoder_vq.encoder.mid.attn_1.k.bias, encoder_vq.encoder.mid.attn_1.v.weight, encoder_vq.encoder.mid.attn_1.v.bias, encoder_vq.encoder.mid.attn_1.proj_out.weight, encoder_vq.encoder.mid.attn_1.proj_out.bias, encoder_vq.encoder.mid.block_2.norm1.weight, encoder_vq.encoder.mid.block_2.norm1.bias, encoder_vq.encoder.mid.block_2.conv1.weight, encoder_vq.encoder.mid.block_2.conv1.bias, encoder_vq.encoder.mid.block_2.norm2.weight, encoder_vq.encoder.mid.block_2.norm2.bias, encoder_vq.encoder.mid.block_2.conv2.weight, encoder_vq.encoder.mid.block_2.conv2.bias, encoder_vq.encoder.norm_out.weight, encoder_vq.encoder.norm_out.bias, encoder_vq.encoder.conv_out.weight, encoder_vq.encoder.conv_out.bias, encoder_vq.quant_conv.weight, encoder_vq.quant_conv.bias, encoder_vq.post_quant_conv.weight, encoder_vq.post_quant_conv.bias, unet.frozen_unet.diffusion_model.time_embed.0.weight, unet.frozen_unet.diffusion_model.time_embed.0.bias, unet.frozen_unet.diffusion_model.time_embed.2.weight, unet.frozen_unet.diffusion_model.time_embed.2.bias, unet.frozen_unet.diffusion_model.input_blocks.0.0.weight, unet.frozen_unet.diffusion_model.input_blocks.0.0.bias, unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.3.0.op.weight, unet.frozen_unet.diffusion_model.input_blocks.3.0.op.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.weight, unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.6.0.op.weight, unet.frozen_unet.diffusion_model.input_blocks.6.0.op.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.weight, unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.weight, unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.bias, unet.frozen_unet.diffusion_model.input_blocks.9.0.op.weight, unet.frozen_unet.diffusion_model.input_blocks.9.0.op.bias, unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.middle_block.1.norm.weight, unet.frozen_unet.diffusion_model.middle_block.1.norm.bias, unet.frozen_unet.diffusion_model.middle_block.1.proj_in.weight, unet.frozen_unet.diffusion_model.middle_block.1.proj_in.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.middle_block.1.proj_out.weight, unet.frozen_unet.diffusion_model.middle_block.1.proj_out.bias, unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.weight, unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.bias, unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.weight, unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.bias, unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.weight, unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.bias, unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.weight, unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.bias, unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.weight, unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.weight, unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.weight, unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.weight, unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias, unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.weight, unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias, unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.weight, unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.bias, unet.frozen_unet.diffusion_model.out.0.weight, unet.frozen_unet.diffusion_model.out.0.bias, unet.frozen_unet.diffusion_model.out.2.weight, unet.frozen_unet.diffusion_model.out.2.bias, unet.trainable_unet.diffusion_model.time_embed.0.weight, unet.trainable_unet.diffusion_model.time_embed.0.bias, unet.trainable_unet.diffusion_model.time_embed.2.weight, unet.trainable_unet.diffusion_model.time_embed.2.bias, unet.trainable_unet.diffusion_model.input_blocks.0.0.weight, unet.trainable_unet.diffusion_model.input_blocks.0.0.bias, unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.3.0.op.weight, unet.trainable_unet.diffusion_model.input_blocks.3.0.op.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.weight, unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.6.0.op.weight, unet.trainable_unet.diffusion_model.input_blocks.6.0.op.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.weight, unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.weight, unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.bias, unet.trainable_unet.diffusion_model.input_blocks.9.0.op.weight, unet.trainable_unet.diffusion_model.input_blocks.9.0.op.bias, unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.middle_block.1.norm.weight, unet.trainable_unet.diffusion_model.middle_block.1.norm.bias, unet.trainable_unet.diffusion_model.middle_block.1.proj_in.weight, unet.trainable_unet.diffusion_model.middle_block.1.proj_in.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.middle_block.1.proj_out.weight, unet.trainable_unet.diffusion_model.middle_block.1.proj_out.bias, unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.weight, unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.bias, unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.weight, unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.bias, unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.weight, unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.bias, unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.weight, unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.bias, unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.weight, unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.weight, unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.weight, unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.weight, unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias, unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.weight, unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias, unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.weight, unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.bias, unet.trainable_unet.diffusion_model.out.0.weight, unet.trainable_unet.diffusion_model.out.0.bias, unet.trainable_unet.diffusion_model.out.2.weight, unet.trainable_unet.diffusion_model.out.2.bias, unet.zero_convs.0.weight, unet.zero_convs.0.bias, unet.zero_convs.1.weight, unet.zero_convs.1.bias, unet.zero_convs.2.weight, unet.zero_convs.2.bias, unet.zero_convs.3.weight, unet.zero_convs.3.bias, unet.zero_convs.4.weight, unet.zero_convs.4.bias, unet.zero_convs.5.weight, unet.zero_convs.5.bias, box_encoder.out64.0.weight, box_encoder.out64.0.bias, box_encoder.out32.0.weight, box_encoder.out32.0.bias, box_encoder.out16.0.weight, box_encoder.out16.0.bias, box_encoder.conv64.0.weight, box_encoder.conv64.0.bias, box_encoder.conv64.2.weight, box_encoder.conv64.2.bias, box_encoder.conv64.4.weight, box_encoder.conv64.4.bias, box_encoder.conv32.0.weight, box_encoder.conv32.0.bias, box_encoder.conv16.0.weight, box_encoder.conv16.0.bias

2025-04-17 09:42:37,963 - mmseg - INFO - Start running, host: s203557@n-62-18-13, work_dir: /zhome/b6/d/154958/ADLCV_Project/VPD/work_dirs/vpd_config
2025-04-17 09:42:37,963 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2025-04-17 09:42:37,963 - mmseg - INFO - workflow: [('train', 1)], max: 80000 iters
2025-04-17 09:42:37,964 - mmseg - INFO - Checkpoints will be saved to /zhome/b6/d/154958/ADLCV_Project/VPD/work_dirs/vpd_config by HardDiskBackend.
2025-04-17 09:43:27,082 - mmseg - INFO - Iter [50/80000]	lr: 2.612e-06, eta: 21:22:05, time: 0.962, data_time: 0.012, memory: 74427, decode.loss_ce: 3.4212, decode.acc_seg: 24.4132, loss: 3.4212
2025-04-17 09:43:51,323 - mmseg - INFO - Iter [100/80000]	lr: 5.274e-06, eta: 16:03:25, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 3.1821, decode.acc_seg: 28.0053, loss: 3.1821
2025-04-17 09:44:15,491 - mmseg - INFO - Iter [150/80000]	lr: 7.932e-06, eta: 14:16:18, time: 0.483, data_time: 0.007, memory: 74427, decode.loss_ce: 3.1223, decode.acc_seg: 27.9654, loss: 3.1223
2025-04-17 09:44:39,743 - mmseg - INFO - Iter [200/80000]	lr: 1.059e-05, eta: 13:23:05, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 2.8706, decode.acc_seg: 30.5295, loss: 2.8706
2025-04-17 09:45:04,027 - mmseg - INFO - Iter [250/80000]	lr: 1.324e-05, eta: 12:51:10, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 2.9434, decode.acc_seg: 28.5232, loss: 2.9434
2025-04-17 09:45:28,307 - mmseg - INFO - Iter [300/80000]	lr: 1.589e-05, eta: 12:29:45, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 3.2213, decode.acc_seg: 29.9863, loss: 3.2213
2025-04-17 09:45:52,769 - mmseg - INFO - Iter [350/80000]	lr: 1.853e-05, eta: 12:15:01, time: 0.489, data_time: 0.007, memory: 74427, decode.loss_ce: 2.6730, decode.acc_seg: 33.7091, loss: 2.6730
2025-04-17 09:46:17,579 - mmseg - INFO - Iter [400/80000]	lr: 2.117e-05, eta: 12:05:01, time: 0.496, data_time: 0.009, memory: 74427, decode.loss_ce: 2.7779, decode.acc_seg: 30.1381, loss: 2.7779
2025-04-17 09:46:42,377 - mmseg - INFO - Iter [450/80000]	lr: 2.381e-05, eta: 11:57:07, time: 0.496, data_time: 0.008, memory: 74427, decode.loss_ce: 2.3272, decode.acc_seg: 36.8525, loss: 2.3272
2025-04-17 09:47:07,186 - mmseg - INFO - Iter [500/80000]	lr: 2.645e-05, eta: 11:50:44, time: 0.496, data_time: 0.008, memory: 74427, decode.loss_ce: 2.6682, decode.acc_seg: 32.7209, loss: 2.6682
2025-04-17 09:47:31,954 - mmseg - INFO - Iter [550/80000]	lr: 2.908e-05, eta: 11:45:21, time: 0.495, data_time: 0.008, memory: 74427, decode.loss_ce: 2.3017, decode.acc_seg: 38.2436, loss: 2.3017
2025-04-17 09:47:56,414 - mmseg - INFO - Iter [600/80000]	lr: 3.171e-05, eta: 11:40:06, time: 0.489, data_time: 0.007, memory: 74427, decode.loss_ce: 2.4493, decode.acc_seg: 36.9780, loss: 2.4493
2025-04-17 09:48:20,703 - mmseg - INFO - Iter [650/80000]	lr: 3.433e-05, eta: 11:35:16, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 2.1238, decode.acc_seg: 40.6590, loss: 2.1238
2025-04-17 09:48:45,020 - mmseg - INFO - Iter [700/80000]	lr: 3.695e-05, eta: 11:31:06, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 2.2855, decode.acc_seg: 41.4471, loss: 2.2855
2025-04-17 09:49:09,352 - mmseg - INFO - Iter [750/80000]	lr: 3.957e-05, eta: 11:27:28, time: 0.487, data_time: 0.007, memory: 74427, decode.loss_ce: 2.1158, decode.acc_seg: 45.9524, loss: 2.1158
2025-04-17 09:49:33,677 - mmseg - INFO - Iter [800/80000]	lr: 4.219e-05, eta: 11:24:14, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 2.0145, decode.acc_seg: 45.0101, loss: 2.0145
2025-04-17 09:49:58,003 - mmseg - INFO - Iter [850/80000]	lr: 4.480e-05, eta: 11:21:20, time: 0.487, data_time: 0.007, memory: 74427, decode.loss_ce: 1.6584, decode.acc_seg: 52.6884, loss: 1.6584
2025-04-17 09:50:22,316 - mmseg - INFO - Iter [900/80000]	lr: 4.741e-05, eta: 11:18:41, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.7747, decode.acc_seg: 51.5140, loss: 1.7747
2025-04-17 09:50:46,624 - mmseg - INFO - Iter [950/80000]	lr: 5.001e-05, eta: 11:16:16, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.7375, decode.acc_seg: 55.3054, loss: 1.7375
2025-04-17 09:51:10,923 - mmseg - INFO - Exp name: vpd_config.py
2025-04-17 09:51:10,924 - mmseg - INFO - Iter [1000/80000]	lr: 5.261e-05, eta: 11:14:03, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.9231, decode.acc_seg: 49.1453, loss: 1.9231
2025-04-17 09:51:35,186 - mmseg - INFO - Iter [1050/80000]	lr: 5.521e-05, eta: 11:11:57, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.7646, decode.acc_seg: 51.4696, loss: 1.7646
2025-04-17 09:51:59,451 - mmseg - INFO - Iter [1100/80000]	lr: 5.781e-05, eta: 11:10:00, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.6521, decode.acc_seg: 53.3114, loss: 1.6521
2025-04-17 09:52:23,739 - mmseg - INFO - Iter [1150/80000]	lr: 6.040e-05, eta: 11:08:13, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.5433, decode.acc_seg: 56.7939, loss: 1.5433
2025-04-17 09:52:47,999 - mmseg - INFO - Iter [1200/80000]	lr: 6.299e-05, eta: 11:06:31, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.5800, decode.acc_seg: 55.4876, loss: 1.5800
2025-04-17 09:53:12,308 - mmseg - INFO - Iter [1250/80000]	lr: 6.557e-05, eta: 11:04:59, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.2726, decode.acc_seg: 60.4826, loss: 1.2726
2025-04-17 09:53:36,624 - mmseg - INFO - Iter [1300/80000]	lr: 6.816e-05, eta: 11:03:32, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.3987, decode.acc_seg: 60.4408, loss: 1.3987
2025-04-17 09:54:00,948 - mmseg - INFO - Iter [1350/80000]	lr: 7.073e-05, eta: 11:02:10, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.4560, decode.acc_seg: 54.4404, loss: 1.4560
2025-04-17 09:54:25,262 - mmseg - INFO - Iter [1400/80000]	lr: 7.331e-05, eta: 11:00:52, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.3918, decode.acc_seg: 59.6612, loss: 1.3918
2025-04-17 09:54:49,563 - mmseg - INFO - Iter [1450/80000]	lr: 7.588e-05, eta: 10:59:37, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.2248, decode.acc_seg: 63.0698, loss: 1.2248
2025-04-17 09:55:13,839 - mmseg - INFO - Iter [1500/80000]	lr: 7.845e-05, eta: 10:58:23, time: 0.486, data_time: 0.007, memory: 74427, decode.loss_ce: 1.1194, decode.acc_seg: 63.4243, loss: 1.1194
2025-04-17 09:55:38,096 - mmseg - INFO - Iter [1550/80000]	lr: 7.845e-05, eta: 10:57:12, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.3308, decode.acc_seg: 60.3336, loss: 1.3308
2025-04-17 09:56:02,350 - mmseg - INFO - Iter [1600/80000]	lr: 7.840e-05, eta: 10:56:04, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.1899, decode.acc_seg: 62.4877, loss: 1.1899
2025-04-17 09:56:26,561 - mmseg - INFO - Iter [1650/80000]	lr: 7.835e-05, eta: 10:54:57, time: 0.484, data_time: 0.007, memory: 74427, decode.loss_ce: 0.8608, decode.acc_seg: 72.3569, loss: 0.8608
2025-04-17 09:56:50,814 - mmseg - INFO - Iter [1700/80000]	lr: 7.830e-05, eta: 10:53:54, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.0234, decode.acc_seg: 66.2261, loss: 1.0234
2025-04-17 09:57:15,048 - mmseg - INFO - Iter [1750/80000]	lr: 7.825e-05, eta: 10:52:52, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 0.9383, decode.acc_seg: 70.5668, loss: 0.9383
2025-04-17 09:57:39,285 - mmseg - INFO - Iter [1800/80000]	lr: 7.820e-05, eta: 10:51:52, time: 0.485, data_time: 0.007, memory: 74427, decode.loss_ce: 1.2920, decode.acc_seg: 61.1932, loss: 1.2920
