2025-04-18 16:21:26,903 - mmseg - INFO - Multi-processing start method is `None`
2025-04-18 16:21:26,991 - mmseg - INFO - OpenCV num_threads is `1
2025-04-18 16:21:26,991 - mmseg - INFO - OMP num threads is 1
2025-04-18 16:21:27,042 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-16GB
CUDA_HOME: None
GCC: gcc (GCC) 12.3.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.6.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.29.0+45d03f6
------------------------------------------------------------

2025-04-18 16:21:27,043 - mmseg - INFO - Distributed training: False
2025-04-18 16:21:27,530 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='VPDSeg',
    pretrained='open-mmlab://resnet50_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 1, 1),
        strides=(1, 2, 2, 2),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        style='pytorch',
        contract_dilation=True),
    neck=dict(
        type='FPN',
        in_channels=[320, 790, 1430, 1280],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=256,
        dropout_ratio=0.1,
        num_classes=150,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='slide', crop_size=(512, 512), stride=(341, 341)),
    sd_path='/work3/s203557/checkpoints/v1-5-pruned-emaonly.ckpt',
    sd_config=
    '/zhome/b6/d/154958/ADLCV_Project/VPD/segmentation/v1-inference.yaml')
dataset_type = 'ADE20KDataset'
data_root = '/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016'
IMG_MEAN = [127.5, 127.5, 127.5]
IMG_VAR = [127.5, 127.5, 127.5]
img_norm_cfg = dict(
    mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[127.5, 127.5, 127.5],
        std=[127.5, 127.5, 127.5],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='GenerateBoundingBoxMasksFromSeg'),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg', 'gt_bbox_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=8,
    train=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='GenerateBoundingBoxMasksFromSeg'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_bbox_masks'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/dtu/blackhole/0e/154958/data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/work3/s203557/checkpoints/vpd.chkpt'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
find_unused_parameters = True
optimizer = dict(
    type='AdamW',
    lr=8e-05,
    weight_decay=0.001,
    paramwise_cfg=dict(
        custom_keys=dict(
            unet=dict(lr_mult=0.1),
            encoder_vq=dict(lr_mult=0.0),
            text_encoder=dict(lr_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    power=1,
    min_lr=0.0,
    by_epoch=False,
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=8000)
evaluation = dict(interval=8000, metric='mIoU')
work_dir = './work_dirs/vpd_config'
gpu_ids = [0]
auto_resume = False

2025-04-18 16:21:27,531 - mmseg - INFO - Set random seed to 1247008256, deterministic: False
2025-04-18 16:21:40,238 - mmseg - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-18 16:21:40,260 - mmseg - INFO - initialize FPNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

gamma - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_in.weight - torch.Size([128, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_in.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.0.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv1.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.block.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.downsample.conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.0.downsample.conv.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv1.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.conv2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.nin_shortcut.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.0.nin_shortcut.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.block.1.conv2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.downsample.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.1.downsample.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv1.weight - torch.Size([512, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.nin_shortcut.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.0.nin_shortcut.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.block.1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.downsample.conv.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.2.downsample.conv.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.0.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.down.3.block.1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_1.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.q.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.q.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.k.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.k.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.v.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.v.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.proj_out.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.attn_1.proj_out.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv1.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.mid.block_2.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.norm_out.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.norm_out.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_out.weight - torch.Size([8, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.encoder.conv_out.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.quant_conv.weight - torch.Size([8, 8, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.quant_conv.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.post_quant_conv.weight - torch.Size([4, 4, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

encoder_vq.post_quant_conv.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.0.weight - torch.Size([1280, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.2.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.time_embed.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.0.0.weight - torch.Size([320, 4, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.1.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.2.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.3.0.op.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.3.0.op.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.weight - torch.Size([640, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.4.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.5.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.6.0.op.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.6.0.op.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.weight - torch.Size([1280, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.7.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.8.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.9.0.op.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.9.0.op.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.middle_block.2.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.0.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.1.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.2.1.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.3.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.4.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight - torch.Size([1280, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.weight - torch.Size([1280, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.5.2.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight - torch.Size([640, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.weight - torch.Size([640, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.6.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight - torch.Size([640, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.weight - torch.Size([640, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.7.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight - torch.Size([640, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.weight - torch.Size([640, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.8.2.conv.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight - torch.Size([320, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.weight - torch.Size([320, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.9.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.10.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.output_blocks.11.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.2.weight - torch.Size([4, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.frozen_unet.diffusion_model.out.2.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.0.weight - torch.Size([1280, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.2.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.time_embed.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.0.0.weight - torch.Size([320, 4, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.1.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.2.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.3.0.op.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.3.0.op.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.weight - torch.Size([640, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.4.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.5.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.6.0.op.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.6.0.op.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.weight - torch.Size([1280, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.7.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.8.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.9.0.op.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.9.0.op.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.10.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.input_blocks.11.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.middle_block.2.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.0.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.1.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.2.1.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.3.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.0.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.weight - torch.Size([1280, 2560, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.weight - torch.Size([1280, 2560, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.4.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.weight - torch.Size([1280, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.in_layers.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.emb_layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.out_layers.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.weight - torch.Size([1280, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.0.skip_connection.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.norm.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_in.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([10240, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([10240]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - torch.Size([1280, 5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([1280, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([1280, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.1.proj_out.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.5.2.conv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.0.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.weight - torch.Size([640, 1920, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.weight - torch.Size([640, 1920, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.6.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.weight - torch.Size([640, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.weight - torch.Size([640, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.7.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.weight - torch.Size([640, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.in_layers.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.weight - torch.Size([640, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.emb_layers.1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.out_layers.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.weight - torch.Size([640, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.0.skip_connection.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.norm.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_in.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([5120, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([5120]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight - torch.Size([640, 2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([640, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([640, 640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.1.proj_out.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.8.2.conv.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.0.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.weight - torch.Size([320, 960, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.weight - torch.Size([320, 960, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.9.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.10.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.weight - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.weight - torch.Size([320, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.in_layers.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.emb_layers.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.out_layers.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.weight - torch.Size([320, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.0.skip_connection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_in.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight - torch.Size([2560, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight - torch.Size([320, 1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight - torch.Size([320, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.output_blocks.11.1.proj_out.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.0.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.2.weight - torch.Size([4, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.trainable_unet.diffusion_model.out.2.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.0.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.2.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.2.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.3.weight - torch.Size([640, 640, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.3.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.4.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.4.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.5.weight - torch.Size([1280, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of VPDSeg  

unet.zero_convs.5.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out64.0.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out64.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out32.0.weight - torch.Size([640, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out32.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out16.0.weight - torch.Size([1280, 1280, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.out16.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.0.weight - torch.Size([128, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.2.weight - torch.Size([320, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.4.weight - torch.Size([320, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv64.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv32.0.weight - torch.Size([640, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv32.0.bias - torch.Size([640]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv16.0.weight - torch.Size([1280, 640, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

box_encoder.conv16.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.2.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

text_adapter.fc.2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.0.conv.weight - torch.Size([256, 320, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.1.conv.weight - torch.Size([256, 790, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.2.conv.weight - torch.Size([256, 1430, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.lateral_convs.3.conv.weight - torch.Size([256, 1280, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.conv_seg.weight - torch.Size([150, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([150]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.scale_heads.0.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.0.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.0.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.1.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.2.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  

decode_head.scale_heads.3.4.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of VPDSeg  
